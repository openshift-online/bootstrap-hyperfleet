#!/bin/bash
set -euo pipefail

# bin/cluster-status - Cluster State Comparison Tool
# Compares ACM ManagedClusters with repository configurations to identify mismatches

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" &> /dev/null && pwd)"
ROOT_DIR="$(dirname "$SCRIPT_DIR")"

# Default values
DEBUG=${DEBUG:-false}
OUTPUT_FORMAT="table"  # table|json|csv
SHOW_ONLY_ISSUES=false
HEALTH_LEVEL="basic"  # basic|deep|full|infrastructure|platform|workloads
SPECIFIC_CLUSTER=""
PARALLEL_CHECKS=5
CLUSTER_TIMEOUT=30

# Usage information
usage() {
    cat <<EOF
Usage: $0 [OPTIONS]

Comprehensive cluster health assessment across multiple layers:
- Repository vs ACM state comparison (basic)
- Infrastructure health: node capacity, provisioning status (deep+)
- Platform health: cluster operators, core services (deep+)
- Sync wave progression and application status (deep+)
- Workload scheduling and resource constraints (full)
- External dependencies: secrets, vault, registry (full)

HEALTH LEVELS:
    --health-basic       Repository + ACM + ArgoCD validation (default)
    --health-deep        + Infrastructure + Platform validation (recommended)
    --health-full        + Workload + External dependencies (comprehensive)
    --health-infrastructure  Infrastructure layer only
    --health-platform    Platform layer only  
    --health-workloads   Workload layer only

OPTIONS:
    --format FORMAT      Output format: table (default), json, csv
    --issues-only        Show only problematic clusters
    --cluster CLUSTER    Check specific cluster only
    --parallel N         Number of parallel health checks (default: 5)
    --timeout N          Per-cluster timeout in seconds (default: 30)
    --debug              Enable debug output
    --help               Show this help message

EXAMPLES:
    $0                              # Basic cluster status (GitOps layer only)
    $0 --health-deep                # Infrastructure + Platform health (recommended)
    $0 --health-full                # Comprehensive health (all layers)
    $0 --health-deep --issues-only  # Show only problematic clusters with deep health
    $0 --health-full --cluster ocp-01-mturansk-a3  # Full health for specific cluster
    $0 --health-infrastructure      # Infrastructure health only
    $0 --health-deep --format json  # JSON output with deep health for automation
    $0 --health-full --format csv > cluster-health.csv  # CSV export with full health

OUTPUT:
    - Comparison table showing repository vs ACM state
    - Identification of stuck terminations, finalizers, orphaned resources
    - ArgoCD Application status for each cluster
    - Recommended cleanup actions

EOF
}

# Parse command line arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        --health-basic)
            HEALTH_LEVEL="basic"
            shift
            ;;
        --health-deep)
            HEALTH_LEVEL="deep"
            shift
            ;;
        --health-full)
            HEALTH_LEVEL="full"
            shift
            ;;
        --health-infrastructure)
            HEALTH_LEVEL="infrastructure"
            shift
            ;;
        --health-platform)
            HEALTH_LEVEL="platform"
            shift
            ;;
        --health-workloads)
            HEALTH_LEVEL="workloads"
            shift
            ;;
        --format)
            OUTPUT_FORMAT="$2"
            shift 2
            ;;
        --issues-only)
            SHOW_ONLY_ISSUES=true
            shift
            ;;
        --cluster)
            SPECIFIC_CLUSTER="$2"
            shift 2
            ;;
        --parallel)
            PARALLEL_CHECKS="$2"
            shift 2
            ;;
        --timeout)
            CLUSTER_TIMEOUT="$2"
            shift 2
            ;;
        --debug)
            DEBUG=true
            set -x
            shift
            ;;
        --help)
            usage
            exit 0
            ;;
        *)
            echo "Unknown option: $1"
            usage
            exit 1
            ;;
    esac
done

# Debug logging function
debug_log() {
    if [[ "$DEBUG" == "true" ]]; then
        echo "[DEBUG $(date '+%H:%M:%S')] $*" >&2
    fi
}

# Check dependencies
check_dependencies() {
    debug_log "Checking dependencies..."
    
    local missing_deps=()
    
    command -v oc >/dev/null 2>&1 || missing_deps+=("oc")
    command -v jq >/dev/null 2>&1 || missing_deps+=("jq")
    
    if [[ ${#missing_deps[@]} -gt 0 ]]; then
        echo "‚ùå Error: Missing required dependencies: ${missing_deps[*]}"
        echo "   Install missing tools and try again"
        exit 1
    fi
    
    # Check OpenShift connection
    if ! oc whoami >/dev/null 2>&1; then
        echo "‚ùå Error: Not connected to OpenShift cluster"
        echo "   Run: oc login"
        exit 1
    fi
    
    debug_log "All dependencies satisfied"
}

# Get clusters from repository
get_repository_clusters() {
    debug_log "Discovering clusters from repository..."
    
    local clusters=()
    
    # If specific cluster requested, only check that one
    if [[ -n "$SPECIFIC_CLUSTER" ]]; then
        echo "$SPECIFIC_CLUSTER"
        return
    fi
    
    # Scan clusters/ directory for deployed clusters
    if [[ -d "$ROOT_DIR/clusters/" ]]; then
        while IFS= read -r -d '' cluster_dir; do
            local cluster_name=$(basename "$cluster_dir")
            clusters+=("$cluster_name")
        done < <(find "$ROOT_DIR/clusters/" -mindepth 1 -maxdepth 1 -type d -print0 2>/dev/null || true)
    fi
    
    # Scan regions/ directory for regional specifications
    if [[ -d "$ROOT_DIR/regions/" ]]; then
        while IFS= read -r -d '' region_dir; do
            local region_name=$(basename "$region_dir")
            while IFS= read -r -d '' cluster_dir; do
                local cluster_name=$(basename "$cluster_dir")
                # Avoid duplicates
                if [[ ! " ${clusters[*]} " =~ " ${cluster_name} " ]]; then
                    clusters+=("$cluster_name")
                fi
            done < <(find "$region_dir" -mindepth 1 -maxdepth 1 -type d -print0 2>/dev/null || true)
        done < <(find "$ROOT_DIR/regions/" -mindepth 1 -maxdepth 1 -type d -print0 2>/dev/null || true)
    fi
    
    printf '%s\n' "${clusters[@]}" | sort -u
}

# Get ManagedClusters from ACM
get_managed_clusters() {
    debug_log "Getting ManagedClusters from ACM..."
    
    if ! oc get managedclusters >/dev/null 2>&1; then
        echo "Warning: Cannot access ManagedClusters (ACM not available?)" >&2
        return 0
    fi
    
    oc get managedclusters -o json | jq -r '.items[] | select(.metadata.name != "local-cluster") | .metadata.name' | sort
}

# Get cluster status details
get_cluster_status() {
    local cluster_name="$1"
    
    # Get ManagedCluster status
    local mc_status="Not Found"
    local mc_available="N/A"
    local mc_finalizers=""
    local mc_taint=""
    
    if oc get managedcluster "$cluster_name" >/dev/null 2>&1; then
        mc_status="Exists"
        mc_available=$(oc get managedcluster "$cluster_name" -o jsonpath='{.status.conditions[?(@.type=="ManagedClusterConditionAvailable")].status}' 2>/dev/null || echo "Unknown")
        
        # Check for finalizers
        local finalizers=$(oc get managedcluster "$cluster_name" -o jsonpath='{.metadata.finalizers[*]}' 2>/dev/null || echo "")
        if [[ -n "$finalizers" ]]; then
            mc_finalizers="Yes"
        else
            mc_finalizers="No"
        fi
        
        # Check for taints
        local taints=$(oc get managedcluster "$cluster_name" -o jsonpath='{.spec.taints[*].effect}' 2>/dev/null || echo "")
        if [[ -n "$taints" ]]; then
            mc_taint="$taints"
        else
            mc_taint="None"
        fi
    fi
    
    # Check namespace status
    local ns_status="Not Found"
    if oc get namespace "$cluster_name" >/dev/null 2>&1; then
        local phase=$(oc get namespace "$cluster_name" -o jsonpath='{.status.phase}' 2>/dev/null || echo "Unknown")
        ns_status="$phase"
    fi
    
    # Check repository configuration
    local repo_config="No"
    if [[ -d "$ROOT_DIR/clusters/$cluster_name" ]] || find "$ROOT_DIR/regions/" -name "$cluster_name" -type d >/dev/null 2>&1; then
        repo_config="Yes"
    fi
    
    # Check ArgoCD Applications
    local argo_apps="N/A"
    if oc get applications.argoproj.io -A >/dev/null 2>&1; then
        argo_apps=$(oc get applications.argoproj.io -A -o json 2>/dev/null | jq -r --arg cluster "$cluster_name" '.items[] | select(.metadata.name | contains($cluster)) | .metadata.name' 2>/dev/null | wc -l || echo "0")
    fi
    
    echo "$cluster_name|$repo_config|$mc_status|$mc_available|$mc_finalizers|$mc_taint|$ns_status|$argo_apps"
}

# Determine if cluster has issues
has_issues() {
    local cluster_data="$1"
    IFS='|' read -r cluster repo_config mc_status mc_available mc_finalizers mc_taint ns_status argo_apps <<< "$cluster_data"
    
    # Issue conditions:
    # 1. ManagedCluster exists but no repo config (orphaned)
    # 2. Repo config exists but no ManagedCluster (missing)
    # 3. Namespace stuck in Terminating
    # 4. ManagedCluster has finalizers but is unavailable
    # 5. ManagedCluster has taints
    
    if [[ "$mc_status" == "Exists" && "$repo_config" == "No" ]]; then
        return 0  # Orphaned ManagedCluster
    fi
    
    if [[ "$repo_config" == "Yes" && "$mc_status" == "Not Found" ]]; then
        return 0  # Missing ManagedCluster
    fi
    
    if [[ "$ns_status" == "Terminating" ]]; then
        return 0  # Stuck namespace
    fi
    
    if [[ "$mc_finalizers" == "Yes" && "$mc_available" != "True" ]]; then
        return 0  # Stuck finalizers
    fi
    
    if [[ "$mc_taint" != "None" ]]; then
        return 0  # Has taints
    fi
    
    return 1  # No issues
}

# Infrastructure health check
check_infrastructure_health() {
    local cluster_name="$1"
    
    debug_log "Checking infrastructure health for $cluster_name..."
    
    # Get expected worker count from MachinePool
    local expected_workers="N/A"
    if oc get machinepool -n "$cluster_name" "${cluster_name}-worker" >/dev/null 2>&1; then
        expected_workers=$(oc get machinepool -n "$cluster_name" "${cluster_name}-worker" -o jsonpath='{.spec.replicas}' 2>/dev/null || echo "N/A")
    fi
    
    # Get actual worker count (requires cluster access)
    local actual_workers="N/A"
    local ready_workers="N/A"
    if timeout "$CLUSTER_TIMEOUT" oc --context="$cluster_name" get nodes >/dev/null 2>&1; then
        actual_workers=$(oc --context="$cluster_name" get nodes --no-headers 2>/dev/null | grep -c worker || echo "0")
        ready_workers=$(oc --context="$cluster_name" get nodes --no-headers 2>/dev/null | grep worker | grep -c Ready || echo "0")
    fi
    
    # CAPI cluster status for EKS
    local capi_ready="N/A"
    if oc get cluster.cluster.x-k8s.io "$cluster_name" -n "$cluster_name" >/dev/null 2>&1; then
        capi_ready=$(oc get cluster.cluster.x-k8s.io "$cluster_name" -n "$cluster_name" -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}' 2>/dev/null || echo "Unknown")
    fi
    
    # Hive cluster status for OCP
    local hive_ready="N/A"
    if oc get clusterdeployment "$cluster_name" -n "$cluster_name" >/dev/null 2>&1; then
        hive_ready=$(oc get clusterdeployment "$cluster_name" -n "$cluster_name" -o jsonpath='{.status.conditions[?(@.type=="ClusterReadyCondition")].status}' 2>/dev/null || echo "Unknown")
    fi
    
    echo "$expected_workers/$actual_workers/$ready_workers/$capi_ready/$hive_ready"
}

# Platform health check
check_platform_health() {
    local cluster_name="$1"
    
    debug_log "Checking platform health for $cluster_name..."
    
    # Count degraded cluster operators (requires cluster access)
    local degraded_cos="N/A"
    local api_health="N/A"
    local etcd_health="N/A"
    
    if timeout "$CLUSTER_TIMEOUT" oc --context="$cluster_name" get co >/dev/null 2>&1; then
        degraded_cos=$(oc --context="$cluster_name" get co --no-headers 2>/dev/null | awk '$3 != "True" || $4 != "False" || $5 != "False" {count++} END {print count+0}' || echo "N/A")
        api_health=$(oc --context="$cluster_name" get co kube-apiserver -o jsonpath='{.status.conditions[?(@.type=="Available")].status}' 2>/dev/null || echo "Unknown")
        etcd_health=$(oc --context="$cluster_name" get co etcd -o jsonpath='{.status.conditions[?(@.type=="Available")].status}' 2>/dev/null || echo "Unknown")
    fi
    
    echo "$degraded_cos/$api_health/$etcd_health"
}

# Sync wave health check
check_sync_wave_health() {
    local cluster_name="$1"
    
    debug_log "Checking sync wave health for $cluster_name..."
    
    # Find highest sync wave with applications
    local max_wave=0
    local current_wave=0
    
    if oc get applications.argoproj.io -A >/dev/null 2>&1; then
        for wave in $(seq 1 10); do
            local wave_apps=$(oc get applications.argoproj.io -A -o json 2>/dev/null | jq -r --arg cluster "$cluster_name" --arg wave "$wave" '.items[] | select(.metadata.name | contains($cluster)) | select(.metadata.annotations["argocd.argoproj.io/sync-wave"] == $wave) | .metadata.name' 2>/dev/null | wc -l || echo "0")
            if [[ $wave_apps -gt 0 ]]; then
                max_wave=$wave
                local synced_apps=$(oc get applications.argoproj.io -A -o json 2>/dev/null | jq -r --arg cluster "$cluster_name" --arg wave "$wave" '.items[] | select(.metadata.name | contains($cluster)) | select(.metadata.annotations["argocd.argoproj.io/sync-wave"] == $wave) | select(.status.sync.status == "Synced") | .metadata.name' 2>/dev/null | wc -l || echo "0")
                if [[ $synced_apps -eq $wave_apps ]]; then
                    current_wave=$wave
                fi
            fi
        done
    fi
    
    echo "$current_wave/$max_wave"
}

# Workload health check
check_workload_health() {
    local cluster_name="$1"
    
    debug_log "Checking workload health for $cluster_name..."
    
    # Count pending pods and PVCs (requires cluster access)
    local pending_pods="N/A"
    local pending_pvcs="N/A"
    
    if timeout "$CLUSTER_TIMEOUT" oc --context="$cluster_name" get pods >/dev/null 2>&1; then
        pending_pods=$(oc --context="$cluster_name" get pods -A --field-selector=status.phase=Pending --no-headers 2>/dev/null | wc -l || echo "N/A")
        pending_pvcs=$(oc --context="$cluster_name" get pvc -A --no-headers 2>/dev/null | grep -c Pending || echo "0")
    fi
    
    echo "$pending_pods/$pending_pvcs"
}

# External dependencies health check
check_external_dependencies() {
    local cluster_name="$1"
    
    debug_log "Checking external dependencies for $cluster_name..."
    
    # External Secrets health (requires cluster access)
    local external_secrets_failed="N/A"
    local connectivity="N/A"
    
    if timeout "$CLUSTER_TIMEOUT" oc --context="$cluster_name" get externalsecrets >/dev/null 2>&1; then
        external_secrets_failed=$(oc --context="$cluster_name" get externalsecrets -A -o json 2>/dev/null | jq '[.items[] | select(.status.conditions[] | select(.type == "Ready" and .status != "True"))] | length' 2>/dev/null || echo "N/A")
        connectivity="OK"
    else
        connectivity="FAILED"
    fi
    
    echo "$external_secrets_failed/$connectivity"
}

# ArgoCD health check
check_argocd_health() {
    local cluster_name="$1"
    
    debug_log "Checking ArgoCD health for $cluster_name..."
    
    # Check for stuck applications (with deletion timestamp)
    local stuck_applications="0"
    local stuck_applicationsets="0"
    local app_sync_issues="0"
    
    if oc get applications.argoproj.io -A >/dev/null 2>&1; then
        # Count applications stuck in deletion
        stuck_applications=$(oc get applications.argoproj.io -A -o json 2>/dev/null | jq --arg cluster "$cluster_name" '.items[] | select(.metadata.name | contains($cluster)) | select(.metadata.deletionTimestamp) | .metadata.name' 2>/dev/null | wc -l || echo "0")
        
        # Count applications with sync issues
        app_sync_issues=$(oc get applications.argoproj.io -A -o json 2>/dev/null | jq --arg cluster "$cluster_name" '.items[] | select(.metadata.name | contains($cluster)) | select(.status.sync.status != "Synced") | .metadata.name' 2>/dev/null | wc -l || echo "0")
        
        # Check for stuck applicationsets
        if oc get applicationsets.argoproj.io -A >/dev/null 2>&1; then
            stuck_applicationsets=$(oc get applicationsets.argoproj.io -A -o json 2>/dev/null | jq --arg cluster "$cluster_name" '.items[] | select(.metadata.name | contains($cluster)) | select(.metadata.deletionTimestamp) | .metadata.name' 2>/dev/null | wc -l || echo "0")
        fi
    fi
    
    echo "$stuck_applications/$stuck_applicationsets/$app_sync_issues"
}

# Issue detection functions
detect_infrastructure_issues() {
    local infra_data="$1"
    IFS='/' read -r expected actual ready capi_ready hive_ready <<< "$infra_data"
    
    local issues=()
    
    if [[ "$actual" != "N/A" && "$expected" != "N/A" ]] && (( actual < expected )); then
        issues+=("INSUFFICIENT_WORKERS")
    fi
    
    if [[ "$ready" != "N/A" && "$actual" != "N/A" ]] && (( ready < actual )); then
        issues+=("NODES_NOT_READY")
    fi
    
    if [[ "$capi_ready" == "False" ]] || [[ "$hive_ready" == "False" ]]; then
        issues+=("CLUSTER_NOT_PROVISIONED")
    fi
    
    printf '%s\n' "${issues[@]}"
}

detect_platform_issues() {
    local platform_data="$1"
    IFS='/' read -r degraded_cos api_health etcd_health <<< "$platform_data"
    
    local issues=()
    
    if [[ "$degraded_cos" != "N/A" ]] && (( degraded_cos > 0 )); then
        issues+=("DEGRADED_COS")
    fi
    
    if [[ "$api_health" != "True" ]] || [[ "$etcd_health" != "True" ]]; then
        issues+=("CORE_SERVICES_DOWN")
    fi
    
    printf '%s\n' "${issues[@]}"
}

detect_sync_wave_issues() {
    local sync_data="$1"
    IFS='/' read -r current_wave max_wave <<< "$sync_data"
    
    local issues=()
    
    if [[ "$max_wave" != "N/A" && "$current_wave" != "N/A" ]] && (( current_wave < max_wave )); then
        issues+=("STUCK_SYNC_WAVES")
    fi
    
    printf '%s\n' "${issues[@]}"
}

detect_workload_issues() {
    local workload_data="$1"
    IFS='/' read -r pending_pods pending_pvcs <<< "$workload_data"
    
    local issues=()
    
    if [[ "$pending_pods" != "N/A" ]] && (( pending_pods > 0 )); then
        issues+=("PODS_PENDING")
    fi
    
    if [[ "$pending_pvcs" != "N/A" ]] && (( pending_pvcs > 0 )); then
        issues+=("PVC_BINDING_FAILED")
    fi
    
    printf '%s\n' "${issues[@]}"
}

detect_external_issues() {
    local external_data="$1"
    IFS='/' read -r failed_secrets connectivity <<< "$external_data"
    
    local issues=()
    
    if [[ "$failed_secrets" != "N/A" ]] && (( failed_secrets > 0 )); then
        issues+=("EXTERNAL_SECRETS_FAILED")
    fi
    
    if [[ "$connectivity" == "FAILED" ]]; then
        issues+=("CONNECTIVITY_FAILED")
    fi
    
    printf '%s\n' "${issues[@]}"
}

detect_argocd_issues() {
    local argocd_data="$1"
    IFS='/' read -r stuck_apps stuck_appsets sync_issues <<< "$argocd_data"
    
    local issues=()
    
    if [[ "$stuck_apps" != "N/A" ]] && (( stuck_apps > 0 )); then
        issues+=("STUCK_ARGOCD_APPS")
    fi
    
    if [[ "$stuck_appsets" != "N/A" ]] && (( stuck_appsets > 0 )); then
        issues+=("STUCK_ARGOCD_APPSETS")
    fi
    
    if [[ "$sync_issues" != "N/A" ]] && (( sync_issues > 0 )); then
        issues+=("ARGOCD_SYNC_FAILED")
    fi
    
    printf '%s\n' "${issues[@]}"
}

# Generate issue summary
get_issue_summary() {
    local cluster_data="$1"
    IFS='|' read -r cluster repo_config mc_status mc_available mc_finalizers mc_taint ns_status argo_apps <<< "$cluster_data"
    
    local issues=()
    
    if [[ "$mc_status" == "Exists" && "$repo_config" == "No" ]]; then
        issues+=("ORPHANED_MC")
    fi
    
    if [[ "$repo_config" == "Yes" && "$mc_status" == "Not Found" ]]; then
        issues+=("MISSING_MC")
    fi
    
    if [[ "$ns_status" == "Terminating" ]]; then
        issues+=("STUCK_NS")
    fi
    
    if [[ "$mc_finalizers" == "Yes" && "$mc_available" != "True" ]]; then
        issues+=("STUCK_FINALIZERS")
    fi
    
    if [[ "$mc_taint" != "None" ]]; then
        issues+=("TAINTED")
    fi
    
    if [[ ${#issues[@]} -eq 0 ]]; then
        echo "OK"
    else
        IFS=','
        echo "${issues[*]}"
    fi
}

# Generate table output
generate_table() {
    local cluster_data=("$@")
    
    echo "| Cluster | Repo Config | ManagedCluster | Available | Finalizers | Taints | Namespace | Argo Apps | Issues |"
    echo "|---------|-------------|----------------|-----------|------------|--------|-----------|-----------|--------|"
    
    for data in "${cluster_data[@]}"; do
        IFS='|' read -r cluster repo_config mc_status mc_available mc_finalizers mc_taint ns_status argo_apps <<< "$data"
        
        local issues=$(get_issue_summary "$data")
        
        # Skip if showing only issues and no issues found
        if [[ "$SHOW_ONLY_ISSUES" == "true" && "$issues" == "OK" ]]; then
            continue
        fi
        
        echo "| $cluster | $repo_config | $mc_status | $mc_available | $mc_finalizers | $mc_taint | $ns_status | $argo_apps | $issues |"
    done
}

# Generate JSON output
generate_json() {
    local cluster_data=("$@")
    
    echo '{"clusters":['
    local first=true
    
    for data in "${cluster_data[@]}"; do
        IFS='|' read -r cluster repo_config mc_status mc_available mc_finalizers mc_taint ns_status argo_apps <<< "$data"
        
        local issues=$(get_issue_summary "$data")
        
        # Skip if showing only issues and no issues found
        if [[ "$SHOW_ONLY_ISSUES" == "true" && "$issues" == "OK" ]]; then
            continue
        fi
        
        if [[ "$first" == "false" ]]; then
            echo ","
        fi
        first=false
        
        cat <<EOF
  {
    "name": "$cluster",
    "repository_config": $(if [[ "$repo_config" == "Yes" ]]; then echo "true"; else echo "false"; fi),
    "managed_cluster": {
      "exists": $(if [[ "$mc_status" == "Exists" ]]; then echo "true"; else echo "false"; fi),
      "available": "$mc_available",
      "has_finalizers": $(if [[ "$mc_finalizers" == "Yes" ]]; then echo "true"; else echo "false"; fi),
      "taints": "$mc_taint"
    },
    "namespace_status": "$ns_status",
    "argocd_applications": $argo_apps,
    "issues": "$issues"
  }
EOF
    done
    
    echo ']}'
}

# Generate CSV output
generate_csv() {
    local cluster_data=("$@")
    
    echo "Cluster,RepoConfig,ManagedCluster,Available,Finalizers,Taints,Namespace,ArgoApps,Issues"
    
    for data in "${cluster_data[@]}"; do
        IFS='|' read -r cluster repo_config mc_status mc_available mc_finalizers mc_taint ns_status argo_apps <<< "$data"
        
        local issues=$(get_issue_summary "$data")
        
        # Skip if showing only issues and no issues found
        if [[ "$SHOW_ONLY_ISSUES" == "true" && "$issues" == "OK" ]]; then
            continue
        fi
        
        echo "$cluster,$repo_config,$mc_status,$mc_available,$mc_finalizers,$mc_taint,$ns_status,$argo_apps,$issues"
    done
}

# Main execution function
main() {
    check_dependencies
    
#    echo "=== Cluster Status Comparison ==="
#    echo "Generated: $(date -u '+%Y-%m-%d %H:%M:%S UTC')"
#    echo ""
    
    # Get all clusters from both sources
    local repo_clusters
    local managed_clusters
    repo_clusters=$(get_repository_clusters)
    managed_clusters=$(get_managed_clusters)
    
    # Combine and deduplicate cluster list
    local all_clusters
    all_clusters=$(echo -e "$repo_clusters\n$managed_clusters" | sort -u | grep -v '^$' || true)
    
    if [[ -z "$all_clusters" ]]; then
        echo "No clusters found in repository or ACM"
        exit 0
    fi
    
    # Collect cluster status data
    local cluster_data=()
    while IFS= read -r cluster; do
        [[ -n "$cluster" ]] || continue
        local status_info
        status_info=$(get_cluster_status "$cluster")
        cluster_data+=("$status_info")
    done <<< "$all_clusters"
    
    # Generate output based on format
    case "$OUTPUT_FORMAT" in
        "csv")
            generate_csv "${cluster_data[@]}"
            ;;
        "json")
            generate_json "${cluster_data[@]}"
            ;;
        *)
            generate_table "${cluster_data[@]}"
            ;;
    esac
    
    # Summary statistics
    if [[ "$OUTPUT_FORMAT" == "table" ]]; then
        echo ""
        echo "=== Summary ==="
        local total_clusters=${#cluster_data[@]}
        local issues_count=0
        
        for data in "${cluster_data[@]}"; do
            if has_issues "$data"; then
                ((issues_count++))
            fi
        done
        
        echo "Total clusters: $total_clusters"
        echo "Clusters with issues: $issues_count"
        echo "Healthy clusters: $((total_clusters - issues_count))"
        
        if [[ $issues_count -gt 0 ]]; then
            echo ""
            echo "‚ö†Ô∏è  Run with --issues-only to see only problematic clusters"
            echo ""
            echo "üîß Recommended remediation actions:"
            echo "   ORPHANED_MC: oc delete managedcluster <cluster-name>"
            echo "   MISSING_MC: Create cluster configuration or provision cluster"
            echo "   STUCK_FINALIZERS: oc patch managedcluster <cluster-name> --type=merge -p '{\"metadata\":{\"finalizers\":[]}}''"
            echo "   STUCK_NS: oc get all,rolebindings,secrets -n <cluster-name> # Check remaining resources"
            echo "   TAINTED: oc patch managedcluster <cluster-name> --type=json -p='[{\"op\": \"remove\", \"path\": \"/spec/taints\"}]'"
        fi
    fi
}

# Run main function
main "$@"