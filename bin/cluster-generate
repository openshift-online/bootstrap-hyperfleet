#!/bin/bash
set -e

# Regional Cluster Generator Tool (Phase 2)
# Generates complete Kustomize overlays from minimal regional specifications.

usage() {
    echo "Usage: $0 [--push-to-gitea] <regional-spec-dir>"
    echo "Example: $0 regions/us-east-1/ocp-01/"
    echo "         $0 regions/us-east-1/eks-01/"
    echo "         $0 regions/us-east-1/hcp-01/"
    echo "         $0 --push-to-gitea regions/us-east-1/eks-01/"
    echo ""
    echo "Options:"
    echo "  --push-to-gitea    Push generated configs to internal Gitea server"
    echo ""
    echo "Supported cluster types: ocp (OpenShift), eks (EKS), hcp (HostedControlPlane)"
    exit 1
}

# Parse command line arguments
PUSH_TO_GITEA=false
SPEC_DIR=""

while [[ $# -gt 0 ]]; do
    case $1 in
        --push-to-gitea)
            PUSH_TO_GITEA=true
            shift
            ;;
        -*)
            echo "Unknown option $1"
            usage
            ;;
        *)
            SPEC_DIR="$1"
            shift
            ;;
    esac
done

if [ -z "$SPEC_DIR" ]; then
    usage
fi

SPEC_FILE="$SPEC_DIR/region.yaml"

if [ ! -f "$SPEC_FILE" ]; then
    echo "Error: Regional specification not found at $SPEC_FILE" >&2
    exit 1
fi

# Parse regional specification
CLUSTER_TYPE=$(grep "type:" "$SPEC_FILE" | awk '{print $2}')
CLUSTER_NAME=$(grep "name:" "$SPEC_FILE" | head -1 | awk '{print $2}')
REGION=$(grep "region:" "$SPEC_FILE" | awk '{print $2}')
DOMAIN=$(grep "domain:" "$SPEC_FILE" | awk '{print $2}')
INSTANCE_TYPE=$(grep "instanceType:" "$SPEC_FILE" | awk '{print $2}')
REPLICAS=$(grep "replicas:" "$SPEC_FILE" | awk '{print $2}')
KUBERNETES_VERSION=$(grep -A 1 "kubernetes:" "$SPEC_FILE" | grep "version:" | awk '{print $2}' | tr -d '"')

# Use the cluster name directly from region.yaml
FULL_CLUSTER_NAME="$CLUSTER_NAME"

# Generate known output paths
CLUSTER_OUTPUT_DIR="clusters/$FULL_CLUSTER_NAME"
# Pipeline directories organized by pipeline type
PIPELINES_CLOUD_INFRA_DIR="pipelines/cloud-infrastructure-provisioning/$FULL_CLUSTER_NAME"
DEPLOYMENTS_OUTPUT_DIR="deployments/ocm/$FULL_CLUSTER_NAME"
GITOPS_OUTPUT_DIR="gitops-applications/clusters"

# Defaults
CLUSTER_TYPE=${CLUSTER_TYPE:-"ocp"}
REGION=${REGION:-"us-west-2"}
DOMAIN=${DOMAIN:-"bootstrap.red-chesterfield.com"}
INSTANCE_TYPE=${INSTANCE_TYPE:-"m5.large"}
REPLICAS=${REPLICAS:-3}
KUBERNETES_VERSION=${KUBERNETES_VERSION:-"1.31"}

# For EKS, ensure semantic versioning (remove 'v' prefix if present and ensure format is X.Y)
if [ "$CLUSTER_TYPE" = "eks" ]; then
    KUBERNETES_VERSION=$(echo "$KUBERNETES_VERSION" | sed 's/^v//')
    # Ensure we have semantic version format (X.Y.Z), default to .0 if only X.Y
    if [[ ! "$KUBERNETES_VERSION" =~ \.[0-9]+$ ]]; then
        KUBERNETES_VERSION="${KUBERNETES_VERSION}.0"
    fi
    EKS_VERSION="v${KUBERNETES_VERSION}"  # EKS API expects v prefix
else
    EKS_VERSION="$KUBERNETES_VERSION"
fi

# Create output directories
mkdir -p "$CLUSTER_OUTPUT_DIR"
mkdir -p "$PIPELINES_CLOUD_INFRA_DIR"
mkdir -p "$DEPLOYMENTS_OUTPUT_DIR"

echo "Generating $CLUSTER_TYPE cluster overlay for $FULL_CLUSTER_NAME"
echo "  Cluster overlay: $CLUSTER_OUTPUT_DIR"
echo "  Cloud Infrastructure pipelines: $PIPELINES_CLOUD_INFRA_DIR"
echo "  Deployments overlay: $DEPLOYMENTS_OUTPUT_DIR"
echo "  GitOps applications: $GITOPS_OUTPUT_DIR"

# Generate namespace.yaml
cat > "$CLUSTER_OUTPUT_DIR/namespace.yaml" << EOF
apiVersion: v1
kind: Namespace
metadata:
  name: $FULL_CLUSTER_NAME
  labels:
    name: $FULL_CLUSTER_NAME
EOF

generate_eks_cluster() {
    # Generate cluster.yaml (CAPI Cluster)
    cat > "$CLUSTER_OUTPUT_DIR/cluster.yaml" << EOF
apiVersion: cluster.x-k8s.io/v1beta1
kind: Cluster
metadata:
  name: $FULL_CLUSTER_NAME
  namespace: $FULL_CLUSTER_NAME
  labels:
    cloud: Amazon
    region: $REGION
    vendor: EKS
spec:
  clusterNetwork:
    pods:
      cidrBlocks:
        - 192.168.0.0/16
  infrastructureRef:
    apiVersion: controlplane.cluster.x-k8s.io/v1beta2
    kind: AWSManagedControlPlane
    name: $FULL_CLUSTER_NAME
  controlPlaneRef:
    kind: AWSManagedControlPlane
    apiVersion: controlplane.cluster.x-k8s.io/v1beta2
    name: $FULL_CLUSTER_NAME
EOF

    # Generate awsmanagedcontrolplane.yaml
    cat > "$CLUSTER_OUTPUT_DIR/awsmanagedcontrolplane.yaml" << EOF
apiVersion: controlplane.cluster.x-k8s.io/v1beta2
kind: AWSManagedControlPlane
metadata:
  name: $FULL_CLUSTER_NAME
  namespace: $FULL_CLUSTER_NAME
spec:
  region: $REGION
  sshKeyName: ""
  version: $EKS_VERSION
  baseDomain: $DOMAIN
  vpc:
    availabilityZoneUsageLimit: 2
    availabilityZoneSelection: Ordered
  logging:
    enable: false
  associateOIDCProvider: true
  eksClusterName: $FULL_CLUSTER_NAME
EOF

    # Generate awsmanagedmachinepool.yaml
    cat > "$CLUSTER_OUTPUT_DIR/awsmanagedmachinepool.yaml" << EOF
apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
kind: AWSManagedMachinePool
metadata:
  name: $FULL_CLUSTER_NAME
  namespace: $FULL_CLUSTER_NAME
spec:
  instanceType: $INSTANCE_TYPE
  scaling:
    minSize: 1
    maxSize: 10
    desiredSize: $REPLICAS
  diskSize: 20
  amiType: AL2_x86_64
EOF

    # Generate machinepool.yaml (Critical: Links CAPI Cluster to AWSManagedMachinePool)
    cat > "$CLUSTER_OUTPUT_DIR/machinepool.yaml" << EOF
apiVersion: cluster.x-k8s.io/v1beta1
kind: MachinePool
metadata:
  name: $FULL_CLUSTER_NAME
  namespace: $FULL_CLUSTER_NAME
spec:
  clusterName: $FULL_CLUSTER_NAME
  replicas: $REPLICAS
  template:
    spec:
      bootstrap:
        dataSecretName: ""
      clusterName: $FULL_CLUSTER_NAME
      infrastructureRef:
        apiVersion: infrastructure.cluster.x-k8s.io/v1beta2
        kind: AWSManagedMachinePool
        name: $FULL_CLUSTER_NAME
      version: $KUBERNETES_VERSION
EOF

    # Generate managedcluster.yaml
    cat > "$CLUSTER_OUTPUT_DIR/managedcluster.yaml" << EOF
apiVersion: cluster.open-cluster-management.io/v1
kind: ManagedCluster
metadata:
  name: $FULL_CLUSTER_NAME
  namespace: $FULL_CLUSTER_NAME
  labels:
    name: $FULL_CLUSTER_NAME
    cloud: Amazon
    region: $REGION
    vendor: EKS
spec:
  hubAcceptsClient: true
EOF

    # Generate klusterletaddonconfig.yaml
    cat > "$CLUSTER_OUTPUT_DIR/klusterletaddonconfig.yaml" << EOF
apiVersion: agent.open-cluster-management.io/v1
kind: KlusterletAddonConfig
metadata:
  name: $FULL_CLUSTER_NAME
  namespace: $FULL_CLUSTER_NAME
spec:
  clusterName: $FULL_CLUSTER_NAME
  clusterNamespace: $FULL_CLUSTER_NAME
  clusterLabels:
    name: $FULL_CLUSTER_NAME
    cloud: Amazon
    vendor: EKS
  applicationManager:
    enabled: true
  policyController:
    enabled: true
  searchCollector:
    enabled: true
  certPolicyController:
    enabled: true
  iamPolicyController:
    enabled: true
EOF

    # Generate ExternalSecrets for EKS cluster
    cat > "$CLUSTER_OUTPUT_DIR/external-secrets.yaml" << EOF
---
apiVersion: external-secrets.io/v1
kind: ExternalSecret
metadata:
  name: aws-credentials
  namespace: $FULL_CLUSTER_NAME
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: vault-cluster-store
    kind: ClusterSecretStore
  target:
    name: aws-credentials
    creationPolicy: Owner
  data:
  - secretKey: aws_access_key_id
    remoteRef:
      key: aws-credentials
      property: aws_access_key_id
  - secretKey: aws_secret_access_key
    remoteRef:
      key: aws-credentials
      property: aws_secret_access_key
---
apiVersion: external-secrets.io/v1
kind: ExternalSecret
metadata:
  name: pull-secret
  namespace: $FULL_CLUSTER_NAME
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: vault-cluster-store
    kind: ClusterSecretStore
  target:
    name: pull-secret
    creationPolicy: Owner
    type: kubernetes.io/dockerconfigjson
  data:
  - secretKey: .dockerconfigjson
    remoteRef:
      key: pull-secret
      property: .dockerconfigjson
EOF

    # Note: Klusterlet CRD is NOT generated for cluster deployment
    # The CRD is managed by ACM hub cluster during import process
    # Including it in cluster deployment causes "AlreadyExists" conflicts
    echo "Skipping Klusterlet CRD generation - managed by ACM hub cluster"

    # Generate kustomization.yaml
    cat > "$CLUSTER_OUTPUT_DIR/kustomization.yaml" << EOF
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - namespace.yaml
  - cluster.yaml
  - awsmanagedcontrolplane.yaml
  - awsmanagedmachinepool.yaml
  - machinepool.yaml
  - managedcluster.yaml
  - klusterletaddonconfig.yaml
  - external-secrets.yaml
  - acm-integration-pipeline.yaml

generatorOptions:
  disableNameSuffixHash: true
EOF

    # Generate ACM Integration Pipeline for automated post-provisioning
    cat > "$CLUSTER_OUTPUT_DIR/acm-integration-pipeline.yaml" << EOF
---
apiVersion: tekton.dev/v1beta1
kind: Task
metadata:
  name: eks-acm-integration-$FULL_CLUSTER_NAME
  namespace: $FULL_CLUSTER_NAME
spec:
  params:
  - name: cluster-name
    type: string
    default: "$FULL_CLUSTER_NAME"
  - name: region
    type: string  
    default: "$REGION"
  stepTemplate:
    env:
    - name: AWS_DEFAULT_REGION
      value: \$(params.region)
    volumeMounts:
    - name: aws-credentials
      mountPath: /root/.aws
      readOnly: true
  steps:
  - name: install-tools
    image: registry.redhat.io/ubi9/ubi:latest
    script: |
      #!/bin/bash
      set -e
      echo "Installing required tools..."
      
      # Install kubectl
      curl -LO "https://dl.k8s.io/release/v1.28.0/bin/linux/amd64/kubectl"
      chmod +x kubectl && mv kubectl /usr/local/bin/
      
      # Install AWS CLI
      curl -LO "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip"
      unzip awscli-exe-linux-x86_64.zip && ./aws/install
      
      # Install OpenShift CLI
      curl -L "https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable/openshift-client-linux.tar.gz" | tar -xz -C /usr/local/bin/ oc
      
      echo "Tools installed successfully"
      
  - name: wait-for-eks-cluster
    image: registry.redhat.io/ubi9/ubi:latest
    script: |
      #!/bin/bash
      set -e
      
      CLUSTER_NAME="\$(params.cluster-name)"
      REGION="\$(params.region)"
      
      echo "Waiting for EKS cluster \$CLUSTER_NAME to become ACTIVE..."
      
      TIMEOUT=1800  # 30 minutes
      ELAPSED=0
      while [ \$ELAPSED -lt \$TIMEOUT ]; do
        STATUS=\$(aws eks describe-cluster --name \$CLUSTER_NAME --region \$REGION --query 'cluster.status' --output text 2>/dev/null || echo "CREATING")
        echo "EKS cluster status: \$STATUS (elapsed: \${ELAPSED}s)"
        
        if [ "\$STATUS" = "ACTIVE" ]; then
          echo "EKS cluster is ACTIVE"
          break
        fi
        
        sleep 30
        ELAPSED=\$((ELAPSED + 30))
      done
      
      if [ \$ELAPSED -ge \$TIMEOUT ]; then
        echo "ERROR: EKS cluster did not become ACTIVE within 30 minutes"
        exit 1
      fi
      
  - name: configure-managed-cluster
    image: registry.redhat.io/ubi9/ubi:latest
    script: |
      #!/bin/bash
      set -e
      
      CLUSTER_NAME="\$(params.cluster-name)"
      REGION="\$(params.region)"
      
      echo "Configuring access to managed cluster..."
      aws eks update-kubeconfig --name \$CLUSTER_NAME --region \$REGION --kubeconfig /tmp/managed-kubeconfig
      
      echo "Skipping Klusterlet CRD installation - managed by ACM hub cluster"
      export KUBECONFIG=/tmp/managed-kubeconfig
      
      echo "Waiting for EKS worker nodes to be ready..."
      kubectl wait --for=condition=Ready nodes --all --timeout=600s
      
    volumeMounts:
      
  - name: apply-acm-import
    image: registry.redhat.io/ubi9/ubi:latest
    script: |
      #!/bin/bash
      set -e
      
      CLUSTER_NAME="\$(params.cluster-name)"
      REGION="\$(params.region)"
      
      echo "Extracting ACM import manifest from hub cluster..."
      # Use hub cluster service account token
      export KUBECONFIG=/var/run/secrets/kubernetes.io/serviceaccount/token
      oc get secret \$CLUSTER_NAME-import -n \$CLUSTER_NAME -o jsonpath='{.data.import\.yaml}' | base64 -d > /tmp/import.yaml
      
      echo "Applying ACM import manifest to managed cluster..."
      export KUBECONFIG=/tmp/managed-kubeconfig
      kubectl apply -f /tmp/import.yaml
      
      echo "Monitoring klusterlet deployment..."
      sleep 60
      
  - name: fix-pull-secret
    image: registry.redhat.io/ubi9/ubi:latest
    script: |
      #!/bin/bash
      set -e
      
      CLUSTER_NAME="\$(params.cluster-name)"
      export KUBECONFIG=/tmp/managed-kubeconfig
      
      if kubectl get pods -n open-cluster-management-agent 2>/dev/null | grep -q "ImagePullBackOff\|ErrImagePull"; then
        echo "Fixing image pull secret for klusterlet..."
        
        # Get pull secret from hub cluster
        export KUBECONFIG=/var/run/secrets/kubernetes.io/serviceaccount/token
        oc get secret pull-secret -n openshift-config -o yaml | \
          sed 's/namespace: openshift-config/namespace: open-cluster-management-agent/' | \
          sed 's/name: pull-secret/name: open-cluster-management-image-pull-credentials/' > /tmp/acm-pull-secret.yaml
        
        # Apply to managed cluster
        export KUBECONFIG=/tmp/managed-kubeconfig
        kubectl delete secret open-cluster-management-image-pull-credentials -n open-cluster-management-agent --ignore-not-found=true
        kubectl apply -f /tmp/acm-pull-secret.yaml
        kubectl rollout restart deployment/klusterlet -n open-cluster-management-agent
        
        echo "Pull secret fixed, waiting for klusterlet to restart..."
        sleep 60
      else
        echo "No image pull issues detected"
      fi
      
  - name: verify-acm-integration
    image: registry.redhat.io/ubi9/ubi:latest
    script: |
      #!/bin/bash
      set -e
      
      CLUSTER_NAME="\$(params.cluster-name)"
      
      echo "Verifying ACM integration..."
      export KUBECONFIG=/var/run/secrets/kubernetes.io/serviceaccount/token
      
      TIMEOUT=600  # 10 minutes
      ELAPSED=0
      while [ \$ELAPSED -lt \$TIMEOUT ]; do
        STATUS=\$(oc get managedcluster \$CLUSTER_NAME -o jsonpath='{.status.conditions[?(@.type=="ManagedClusterConditionAvailable")].status}' 2>/dev/null || echo "Unknown")
        echo "ManagedCluster \$CLUSTER_NAME availability status: \$STATUS (elapsed: \${ELAPSED}s)"
        
        if [ "\$STATUS" = "True" ]; then
          echo "SUCCESS: EKS cluster \$CLUSTER_NAME successfully integrated with ACM"
          oc get managedcluster \$CLUSTER_NAME
          exit 0
        fi
        
        sleep 30
        ELAPSED=\$((ELAPSED + 30))
      done
      
      echo "WARNING: ACM integration did not complete within 10 minutes"
      echo "Current ManagedCluster status:"
      oc get managedcluster \$CLUSTER_NAME -o yaml
      exit 0  # Don't fail the pipeline, cluster may still be integrating
      
  volumes:
  - name: aws-credentials
    secret:
      secretName: aws-credentials
---
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  name: $FULL_CLUSTER_NAME-acm-integration
  namespace: $FULL_CLUSTER_NAME
  annotations:
    argocd.argoproj.io/sync-wave: "2"
spec:
  pipelineSpec:
    tasks:
    - name: integrate-with-acm
      taskRef:
        name: eks-acm-integration-$FULL_CLUSTER_NAME
  serviceAccountName: cluster-provisioner
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: cluster-provisioner
  namespace: $FULL_CLUSTER_NAME
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-provisioner-$FULL_CLUSTER_NAME
rules:
- apiGroups: [""]
  resources: ["secrets", "configmaps"]
  verbs: ["get", "list", "create", "update", "patch", "delete"]
- apiGroups: ["cluster.open-cluster-management.io"]
  resources: ["managedclusters"]
  verbs: ["get", "list", "watch", "create", "update", "patch"]
- apiGroups: ["apiextensions.k8s.io"]
  resources: ["customresourcedefinitions"]
  verbs: ["get", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-provisioner-$FULL_CLUSTER_NAME
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-provisioner-$FULL_CLUSTER_NAME
subjects:
- kind: ServiceAccount
  name: cluster-provisioner
  namespace: $FULL_CLUSTER_NAME
EOF
}

generate_hcp_cluster() {
    # Generate hostedcluster.yaml for HyperShift
    cat > "$CLUSTER_OUTPUT_DIR/hostedcluster.yaml" << EOF
apiVersion: hypershift.openshift.io/v1beta1
kind: HostedCluster
metadata:
  name: $FULL_CLUSTER_NAME
  namespace: $FULL_CLUSTER_NAME
  annotations:
    hypershift.openshift.io/pod-security-admission-label-override: privileged
spec:
  release:
    image: "quay.io/openshift-release-dev/ocp-release@sha256:45a396b169974dcbd8aae481c647bf55bcf9f0f8f6222483d407d7cec450928d"
  pullSecret:
    name: pull-secret
  sshKey:
    name: "$FULL_CLUSTER_NAME-ssh-key"
  infrastructureAvailabilityPolicy: SingleReplica
  networking:
    clusterNetwork:
    - cidr: 10.132.0.0/14
    networkType: OVNKubernetes
    serviceNetwork:
    - cidr: 172.31.0.0/16
  platform:
    type: AWS
    aws:
      region: $REGION
      credentialsSecretRef:
        name: aws-credentials
      rolesRef:
        kubeCloudControllerARN: arn:aws:iam::765374464689:role/controllers.cluster-api-provider-aws.sigs.k8s.io
        nodePoolManagementARN: arn:aws:iam::765374464689:role/controllers.cluster-api-provider-aws.sigs.k8s.io
        controlPlaneOperatorARN: arn:aws:iam::765374464689:role/controllers.cluster-api-provider-aws.sigs.k8s.io
        networkARN: arn:aws:iam::765374464689:role/controllers.cluster-api-provider-aws.sigs.k8s.io
        storageARN: arn:aws:iam::765374464689:role/controllers.cluster-api-provider-aws.sigs.k8s.io
        imageRegistryARN: arn:aws:iam::765374464689:role/controllers.cluster-api-provider-aws.sigs.k8s.io
        ingressARN: arn:aws:iam::765374464689:role/controllers.cluster-api-provider-aws.sigs.k8s.io
  infraID: $FULL_CLUSTER_NAME
  dns:
    baseDomain: $DOMAIN
  services:
  - service: APIServer
    servicePublishingStrategy:
      type: LoadBalancer
  - service: OAuthServer
    servicePublishingStrategy:
      type: Route
  - service: OIDC
    servicePublishingStrategy:
      type: None
  - service: Konnectivity
    servicePublishingStrategy:
      type: Route
  - service: Ignition
    servicePublishingStrategy:
      type: Route
EOF

    # Generate NodePool for HCP cluster
    cat > "$CLUSTER_OUTPUT_DIR/nodepool.yaml" << EOF
apiVersion: hypershift.openshift.io/v1beta1
kind: NodePool
metadata:
  name: $FULL_CLUSTER_NAME-nodepool
  namespace: $FULL_CLUSTER_NAME
spec:
  clusterName: $FULL_CLUSTER_NAME
  nodeCount: $REPLICAS
  platform:
    type: AWS
    aws:
      instanceType: $INSTANCE_TYPE
      subnet:
        filters:
        - name: "tag:kubernetes.io/role/elb"
          values: ["1"]
  management:
    autoRepair: true
    upgradeType: Replace
  release:
    image: "quay.io/openshift-release-dev/ocp-release@sha256:45a396b169974dcbd8aae481c647bf55bcf9f0f8f6222483d407d7cec450928d"
EOF

    # Generate SSH key secret
    cat > "$CLUSTER_OUTPUT_DIR/ssh-key-secret.yaml" << EOF
apiVersion: v1
kind: Secret
metadata:
  name: $FULL_CLUSTER_NAME-ssh-key
  namespace: $FULL_CLUSTER_NAME
type: Opaque
data:
  # TODO: Replace with actual base64-encoded SSH public key
  id_rsa.pub: ""
EOF

    # Generate ExternalSecrets for HCP cluster
    cat > "$CLUSTER_OUTPUT_DIR/external-secrets.yaml" << EOF
---
apiVersion: external-secrets.io/v1
kind: ExternalSecret
metadata:
  name: aws-credentials
  namespace: $FULL_CLUSTER_NAME
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: vault-cluster-store
    kind: ClusterSecretStore
  target:
    name: aws-credentials
    creationPolicy: Owner
  data:
  - secretKey: credentials
    remoteRef:
      key: aws-credentials-arn
      property: credentials
---
apiVersion: external-secrets.io/v1
kind: ExternalSecret
metadata:
  name: pull-secret
  namespace: $FULL_CLUSTER_NAME
spec:
  refreshInterval: 1h
  secretStoreRef:
    name: vault-cluster-store
    kind: ClusterSecretStore
  target:
    name: pull-secret
    creationPolicy: Owner
    type: kubernetes.io/dockerconfigjson
  data:
  - secretKey: .dockerconfigjson
    remoteRef:
      key: pull-secret
      property: .dockerconfigjson
EOF

    # Generate klusterletaddonconfig.yaml
    cat > "$CLUSTER_OUTPUT_DIR/klusterletaddonconfig.yaml" << EOF
apiVersion: agent.open-cluster-management.io/v1
kind: KlusterletAddonConfig
metadata:
  name: $FULL_CLUSTER_NAME
  namespace: $FULL_CLUSTER_NAME
spec:
  applicationManager:
    enabled: true
  certPolicyController:
    enabled: true
  clusterLabels:
    cloud: None
    name: $FULL_CLUSTER_NAME
    vendor: OpenShift
    region: hypershift
  clusterName: $FULL_CLUSTER_NAME
  clusterNamespace: $FULL_CLUSTER_NAME
  policyController:
    enabled: true
  searchCollector:
    enabled: true
  iamPolicyController:
    enabled: true
EOF

    # Generate kustomization.yaml for HCP
    cat > "$CLUSTER_OUTPUT_DIR/kustomization.yaml" << EOF
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - namespace.yaml
  - hostedcluster.yaml
  - nodepool.yaml
  - klusterletaddonconfig.yaml
  - ssh-key-secret.yaml
  - external-secrets.yaml

# This will disable name hashing for all generators in this file
generatorOptions:
  disableNameSuffixHash: true

patches:
  - target:
      kind: KlusterletAddonConfig
      version: v1
      group: agent.open-cluster-management.io
    patch: |
      - op: replace
        path: /metadata/namespace
        value: $FULL_CLUSTER_NAME
      - op: replace
        path: /metadata/name
        value: $FULL_CLUSTER_NAME
      - op: replace
        path: /spec/clusterLabels/name
        value: $FULL_CLUSTER_NAME
      - op: replace
        path: /spec/clusterNamespace
        value: $FULL_CLUSTER_NAME
      - op: replace
        path: /spec/clusterName
        value: $FULL_CLUSTER_NAME
EOF
}

generate_ocp_cluster() {
    # Generate install-config.yaml for OpenShift
    cat > "$CLUSTER_OUTPUT_DIR/install-config.yaml" << EOF
apiVersion: v1
metadata:
  name: '$FULL_CLUSTER_NAME'
baseDomain: $DOMAIN
controlPlane:
  architecture: amd64
  hyperthreading: Enabled
  name: master
  replicas: 3
  platform:
    aws:
      rootVolume:
        iops: 4000
        size: 100
        type: io1
      type: $INSTANCE_TYPE
compute:
  - hyperthreading: Enabled
    architecture: amd64
    name: 'worker'
    replicas: $REPLICAS
    platform:
      aws:
        rootVolume:
          iops: 2000
          size: 100
          type: io1
        type: $INSTANCE_TYPE
networking:
  networkType: OVNKubernetes
  clusterNetwork:
    - cidr: 10.128.0.0/14
      hostPrefix: 23
  machineNetwork:
    - cidr: 10.0.0.0/16
  serviceNetwork:
    - 172.30.0.0/16
platform:
  aws:
    region: $REGION
pullSecret: "" # skip, hive will inject based on it's secrets
EOF

    # Generate klusterletaddonconfig.yaml
    cat > "$CLUSTER_OUTPUT_DIR/klusterletaddonconfig.yaml" << EOF
apiVersion: agent.open-cluster-management.io/v1
kind: KlusterletAddonConfig
metadata:
  name: $FULL_CLUSTER_NAME
  namespace: $FULL_CLUSTER_NAME
spec:
  applicationManager:
    enabled: true
  certPolicyController:
    enabled: true
  clusterLabels:
    cloud: Amazon
    name: $FULL_CLUSTER_NAME
    vendor: OpenShift
    region: $REGION
  clusterName: $FULL_CLUSTER_NAME
  clusterNamespace: $FULL_CLUSTER_NAME
  policyController:
    enabled: true
  searchCollector:
    enabled: true
  iamPolicyController:
    enabled: true
EOF

    # Generate kustomization.yaml for OCP (uses base + patches)
    cat > "$CLUSTER_OUTPUT_DIR/kustomization.yaml" << EOF
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
  - namespace.yaml
  - klusterletaddonconfig.yaml
  - ../../bases/clusters

# This will disable name hashing for all generators in this file
generatorOptions:
  disableNameSuffixHash: true

secretGenerator:
  - name: install-config
    namespace: $FULL_CLUSTER_NAME
    files:
      - install-config.yaml

patches:
  - target:
      kind: ClusterDeployment
      version: v1
      group: hive.openshift.io
    patch: |
      - op: replace
        path: /metadata/namespace
        value: $FULL_CLUSTER_NAME
      - op: replace
        path: /metadata/name
        value: $FULL_CLUSTER_NAME
      - op: replace
        path: /spec/clusterName
        value: $FULL_CLUSTER_NAME
      - op: replace
        path: /spec/platform/aws/region
        value: $REGION
  - target:
      kind: ManagedCluster
      version: v1
      group: cluster.open-cluster-management.io
    patch: |
      - op: replace
        path: /metadata/namespace
        value: $FULL_CLUSTER_NAME
      - op: replace
        path: /metadata/name
        value: $FULL_CLUSTER_NAME
      - op: replace
        path: /metadata/labels/name
        value: $FULL_CLUSTER_NAME
      - op: replace
        path: /metadata/labels/region
        value: $REGION
  - target:
      kind: MachinePool
      version: v1
      group: hive.openshift.io
    patch: |
      - op: replace
        path: /metadata/namespace
        value: $FULL_CLUSTER_NAME
      - op: replace
        path: /spec/clusterDeploymentRef/name
        value: $FULL_CLUSTER_NAME
      - op: replace
        path: /metadata/name
        value: $FULL_CLUSTER_NAME-worker
  - target:
      kind: KlusterletAddonConfig
      version: v1
      group: agent.open-cluster-management.io
    patch: |
      - op: replace
        path: /metadata/namespace
        value: $FULL_CLUSTER_NAME
      - op: replace
        path: /metadata/name
        value: $FULL_CLUSTER_NAME
      - op: replace
        path: /spec/clusterLabels/name
        value: $FULL_CLUSTER_NAME
      - op: replace
        path: /spec/clusterNamespace
        value: $FULL_CLUSTER_NAME
      - op: replace
        path: /spec/clusterName
        value: $FULL_CLUSTER_NAME
  - target:
      kind: ExternalSecret
      version: v1
      group: external-secrets.io
      name: aws-credentials
    patch: |
      - op: replace
        path: /metadata/namespace
        value: $FULL_CLUSTER_NAME
  - target:
      kind: ExternalSecret
      version: v1
      group: external-secrets.io
      name: pull-secret
    patch: |
      - op: replace
        path: /metadata/namespace
        value: $FULL_CLUSTER_NAME
      - op: replace
        path: /spec/target/template/type
        value: kubernetes.io/dockerconfigjson
EOF
}

generate_regional_pipelines() {
    # Generate cloud-infrastructure-provisioning pipeline overlay
    cat > "$PIPELINES_CLOUD_INFRA_DIR/kustomization.yaml" << EOF
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: ocm-$FULL_CLUSTER_NAME

commonAnnotations:
  cluster: $FULL_CLUSTER_NAME
  cluster-type: $CLUSTER_TYPE
  version: "v0.0.1"

resources:
  - cloud-infrastructure-provisioning.pipelinerun.yaml

components:
  - ../../../bases/pipelines/cloud-infrastructure-provisioning
EOF

    # Generate cloud-infrastructure-provisioning pipelinerun
    cat > "$PIPELINES_CLOUD_INFRA_DIR/cloud-infrastructure-provisioning.pipelinerun.yaml" << EOF
apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  name: cloud-infrastructure-provisioning-run
  namespace: ocm-$FULL_CLUSTER_NAME
spec:
  pipelineRef:
    name: cloud-infrastructure-provisioning-pipeline
  params:
    - name: cluster-name
      value: $FULL_CLUSTER_NAME
    - name: cloud-provider
      value: $(if [ "$CLUSTER_TYPE" = "ocp" ]; then echo "aws"; else echo "aws"; fi)
    - name: region
      value: $REGION
    - name: instance-type
      value: $INSTANCE_TYPE
    - name: node-count
      value: "$REPLICAS"
  workspaces:
    - name: shared-workspace
      volumeClaimTemplate:
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 1Gi
EOF
}

generate_regional_deployments() {
    # Generate regional deployments overlay
    cat > "$DEPLOYMENTS_OUTPUT_DIR/kustomization.yaml" << EOF
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: ocm-$FULL_CLUSTER_NAME

resources:
  - ../../../bases/ocm
  - namespace.yaml
EOF

    # Generate namespace.yaml for regional deployments
    cat > "$DEPLOYMENTS_OUTPUT_DIR/namespace.yaml" << EOF
apiVersion: v1
kind: Namespace
metadata:
  name: ocm-$FULL_CLUSTER_NAME
  labels:
    name: ocm-$FULL_CLUSTER_NAME
EOF
}

generate_operators() {
    # Create operators directory
    OPERATORS_OUTPUT_DIR="operators/openshift-pipelines/$FULL_CLUSTER_NAME"
    mkdir -p "$OPERATORS_OUTPUT_DIR"
    
    # Generate operators kustomization.yaml
    cat > "$OPERATORS_OUTPUT_DIR/kustomization.yaml" << EOF
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

namespace: ocm-$FULL_CLUSTER_NAME

commonAnnotations:
  cluster: $FULL_CLUSTER_NAME
  cluster-type: $CLUSTER_TYPE
  version: "v0.0.1"

resources:
  - ../global/overlays/pipelines-operator-only
  - namespace.yaml
EOF

    # Generate namespace.yaml for operators
    cat > "$OPERATORS_OUTPUT_DIR/namespace.yaml" << EOF
apiVersion: v1
kind: Namespace
metadata:
  name: ocm-$FULL_CLUSTER_NAME
  labels:
    name: ocm-$FULL_CLUSTER_NAME
EOF
}

generate_gitops_applications() {
    # Generate ApplicationSet for cluster
    cat > "$GITOPS_OUTPUT_DIR/$FULL_CLUSTER_NAME.yaml" << EOF
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: $FULL_CLUSTER_NAME-applications
  namespace: openshift-gitops
spec:
  generators:
  - list:
      elements:
      - component: cluster
        path: clusters/$FULL_CLUSTER_NAME
        destination: https://kubernetes.default.svc
        syncWave: "10"
      - component: operators
        path: operators/openshift-pipelines/$FULL_CLUSTER_NAME
        destination: https://api.$FULL_CLUSTER_NAME.$DOMAIN:6443
        syncWave: "20"
      - component: pipelines-cloud-infrastructure-provisioning
        path: pipelines/cloud-infrastructure-provisioning/$FULL_CLUSTER_NAME
        destination: https://api.$FULL_CLUSTER_NAME.$DOMAIN:6443
        syncWave: "30"
      - component: deployments-ocm
        path: deployments/ocm/$FULL_CLUSTER_NAME
        destination: https://api.$FULL_CLUSTER_NAME.$DOMAIN:6443
        syncWave: "40"
  template:
    metadata:
      name: $FULL_CLUSTER_NAME-{{component}}
      namespace: openshift-gitops
      annotations:
        argocd.argoproj.io/sync-wave: '{{syncWave}}'
    spec:
      project: default
      source:
        repoURL: https://github.com/openshift-online/bootstrap
        path: '{{path}}'
        targetRevision: main
      destination:
        server: '{{destination}}'
      syncPolicy:
        automated:
          selfHeal: true
          allowEmpty: false
        prune: false
EOF

    # ApplicationSet destinations are handled inline above
}

update_clusters_kustomization() {
    local kustomization_file="clusters/kustomization.yaml"
    
    # Check if the cluster is already in the kustomization
    if ! grep -q "\\- $FULL_CLUSTER_NAME/" "$kustomization_file"; then
        echo "Adding $FULL_CLUSTER_NAME to clusters/kustomization.yaml"
        
        # Check if resources is empty array
        if grep -q "^resources: \[\]" "$kustomization_file"; then
            # Replace empty array with list containing new cluster
            sed -i "s/^resources: \[\]/resources:\n  - $FULL_CLUSTER_NAME\//" "$kustomization_file"
        else
            # Append to existing resources list
            cat >> "$kustomization_file" << EOF
  - $FULL_CLUSTER_NAME/
EOF
        fi
    else
        echo "$FULL_CLUSTER_NAME already exists in clusters/kustomization.yaml"
    fi
}

update_gitops_kustomization() {
    local kustomization_file="$GITOPS_OUTPUT_DIR/kustomization.yaml"
    
    # Check if the cluster ApplicationSet is already in the kustomization
    if ! grep -q "\\- \\./$FULL_CLUSTER_NAME\\.yaml" "$kustomization_file"; then
        echo "Adding $FULL_CLUSTER_NAME ApplicationSet to gitops-applications/clusters/kustomization.yaml"
        
        # Find the line number where cluster applications start (after global/eso.application.yaml)
        local insert_line
#        insert_line=$(grep -n "global/eso.application.yaml" "$kustomization_file" | cut -d: -f1)
        insert_line=$(grep -n "# ADD CLUSTERS HERE" "$kustomization_file" | cut -d: -f1)

        if [ -z "$insert_line" ]; then
            echo "Warning: Could not find global/eso.application.yaml in kustomization.yaml, appending to end"
            cat >> "$kustomization_file" << EOF

- ./$FULL_CLUSTER_NAME.yaml
EOF
        else
            # Insert after the global/eso.application.yaml line
            local temp_file=$(mktemp)
            {
                head -n "$insert_line" "$kustomization_file"
                echo ""
                echo "- ./$FULL_CLUSTER_NAME.yaml"
                tail -n +$((insert_line + 1)) "$kustomization_file"
            } > "$temp_file"
            mv "$temp_file" "$kustomization_file"
        fi
    else
        echo "$FULL_CLUSTER_NAME ApplicationSet already exists in gitops-applications/kustomization.yaml"
    fi
}

push_to_gitea() {
    echo ""
    echo "🚀 Pushing generated configurations to internal Gitea server..."
    
    # Gitea configuration
    local GITEA_URL="http://gitea.gitea-system.svc.cluster.local:3000"
    local GITEA_REPO_URL="http://mturansk:acmeprototype321#@gitea.gitea-system.svc.cluster.local:3000/bootstrap/bootstrap.git"
    local GITEA_USER="mturansk"
    local GITEA_PASS="acmeprototype321#"
    
    # Create a temporary job to push to Gitea from inside the cluster
    echo "Creating push job for cluster: $FULL_CLUSTER_NAME"
    
    oc apply -f - <<EOF
apiVersion: batch/v1
kind: Job
metadata:
  name: gitea-push-$FULL_CLUSTER_NAME
  namespace: gitea-system
spec:
  template:
    spec:
      restartPolicy: Never
      containers:
      - name: git-push
        image: alpine/git:latest
        command:
        - /bin/sh
        - -c
        - |
          set -e
          echo "Setting up git configuration..."
          export HOME=/tmp
          mkdir -p /tmp/.config/git
          git config --global user.name "Bootstrap Generator"
          git config --global user.email "bootstrap@cluster.local"
          git config --global init.defaultBranch main
          
          echo "Cloning bootstrap repository..."
          git clone $GITEA_REPO_URL /tmp/bootstrap || {
            echo "Repository doesn't exist, creating new one..."
            mkdir -p /tmp/bootstrap
            cd /tmp/bootstrap
            git init
            git remote add origin $GITEA_REPO_URL
          }
          
          cd /tmp/bootstrap
          
          # Ensure we're on main branch
          git checkout main 2>/dev/null || git checkout -b main
          
          echo "Creating cluster configuration directories..."
          mkdir -p clusters/$FULL_CLUSTER_NAME
          mkdir -p operators/openshift-pipelines/$FULL_CLUSTER_NAME
          mkdir -p pipelines/cloud-infrastructure-provisioning/$FULL_CLUSTER_NAME
          mkdir -p deployments/ocm/$FULL_CLUSTER_NAME
          mkdir -p gitops-applications
          
          echo "Generated cluster files will be added by separate process..."
          echo "Cluster: $FULL_CLUSTER_NAME" > clusters/$FULL_CLUSTER_NAME/README.md
          echo "Type: $CLUSTER_TYPE" >> clusters/$FULL_CLUSTER_NAME/README.md
          echo "Region: $REGION" >> clusters/$FULL_CLUSTER_NAME/README.md
          echo "Generated: \$(date)" >> clusters/$FULL_CLUSTER_NAME/README.md
          
          git add .
          git commit -m "Initialize structure for $FULL_CLUSTER_NAME ($CLUSTER_TYPE)" || echo "No changes to commit"
          
          echo "Pushing to Gitea repository..."
          git push -u origin main
          
          echo "✅ Successfully pushed cluster structure to Gitea"
        env:
        - name: GITEA_REPO_URL
          value: "$GITEA_REPO_URL"
        - name: FULL_CLUSTER_NAME
          value: "$FULL_CLUSTER_NAME"
        - name: CLUSTER_TYPE
          value: "$CLUSTER_TYPE"
        - name: REGION
          value: "$REGION"
EOF
    
    echo "Waiting for push job to complete..."
    if oc wait --for=condition=complete job/gitea-push-$FULL_CLUSTER_NAME -n gitea-system --timeout=120s; then
        echo "✅ Successfully pushed cluster configuration to Gitea"
        echo "Repository URL: $GITEA_URL/bootstrap/bootstrap"
        echo "Branch: main"
        echo "Cluster path: clusters/$FULL_CLUSTER_NAME"
        
        # Clean up the job
        oc delete job gitea-push-$FULL_CLUSTER_NAME -n gitea-system
    else
        echo "❌ Failed to push to Gitea. Check job logs:"
        oc logs job/gitea-push-$FULL_CLUSTER_NAME -n gitea-system
        oc delete job gitea-push-$FULL_CLUSTER_NAME -n gitea-system
        exit 1
    fi
}

# Generate type-specific manifests
if [ "$CLUSTER_TYPE" = "ocp" ]; then
    generate_ocp_cluster
elif [ "$CLUSTER_TYPE" = "eks" ]; then
    generate_eks_cluster
elif [ "$CLUSTER_TYPE" = "hcp" ]; then
    generate_hcp_cluster
else
    echo "Error: Unknown cluster type '$CLUSTER_TYPE'. Supported types: ocp, eks, hcp" >&2
    exit 1
fi

# Generate regional pipelines, operators, and deployments
generate_regional_pipelines
generate_operators
generate_regional_deployments
generate_gitops_applications
update_clusters_kustomization
update_gitops_kustomization

# Push to Gitea if requested
if [ "$PUSH_TO_GITEA" = true ]; then
    push_to_gitea
fi

echo "Generated $CLUSTER_TYPE cluster overlay successfully!"
echo "Files created:"
echo "Cluster files:"
ls -la "$CLUSTER_OUTPUT_DIR/"
echo ""
echo "Operators files:"
ls -la "operators/openshift-pipelines/$FULL_CLUSTER_NAME/"
echo ""
echo "Cloud Infrastructure pipeline files:"
ls -la "$PIPELINES_CLOUD_INFRA_DIR/"
echo ""
echo "Deployments files:"
ls -la "$DEPLOYMENTS_OUTPUT_DIR/"
echo ""
echo "GitOps ApplicationSet:"
ls -la "$GITOPS_OUTPUT_DIR/$FULL_CLUSTER_NAME.yaml"
echo ""
echo "Updated gitops-applications/kustomization.yaml"